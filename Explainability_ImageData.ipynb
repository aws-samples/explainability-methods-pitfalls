{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations in AI: Methods, Stakeholders and Pitfalls\n",
    "<h3 align=\"center\">Image Data</h3>\n",
    "<br>\n",
    "\n",
    "As machine learning models become increasingly adept at solving image tasks (e.g., object classification, detection, segmentation, ...), there is an emerging need to better understand the reasoning behind the predictions. Computer vision tasks differ from other machine learning tasks in that the base features (i.e., pixels - light intensity values in different color channels) are not generally influential individually. Instead, the combination of pixels that form higher-level features like textures, or patterns is something that can be visually recognize and leveraged to create explanations.\n",
    "\n",
    "Explainability methods for image tasks, also known as pixel attribution methods, can be used to identify the pixels that are most important for a model's prediction. These methods can be classified as either gradient-based methods, occlusion- or perturbation-based methods, or a combination of these approaches. \n",
    "\n",
    "Certain explainability methods for image data also require so-called baselines. These baselines help determine which pixels are important to the predicted label by helping simulate absence of information. A baseline should be composed of neutral or uninformative pixel values; for example, black images, white images, or random noise.\n",
    "\n",
    "To visualize explanations for images, so-called saliency maps are used: A saliency map is a heatmap that shows which parts of an image are most important (salient) for a model's prediction. As described above, different methods exist to create saliency maps. Saliency maps are also called sensitivity maps, or pixel attribution maps. \n",
    "\n",
    "---\n",
    "__Problem Statement:__ This notebook shows how to use a pre-trained image classifier to automatically classify animals and object into different categories. This kind of classifier could be used to automatically index images for a easier retrieval. In this notebook, four prevalent approaches for explaining image classification models: Saliency Maps (Vanilla Gradient), Integrated Gradient, SHAP (GradientShap and DeepLiftShap), and counterfactual (adverserial) examples.\n",
    "\n",
    "---\n",
    "__Dataset:__ \n",
    "[ImageNet](https://www.image-net.org/challenges/LSVRC/index.php) is a large-scale hierarchical image database organized according to the WordNet hierarchy. It spans 1,000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images. For the purpose of this hands-on exercise, a subset of ImageNet is introduced that contains one sample image per class (for a total of 1,000 distinct images). \n",
    "\n",
    "Attribution: Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.\n",
    "\n",
    "---\n",
    "<a name=\"0\">__Contents of Notebook:__</a>\n",
    "\n",
    "1. <a href=\"#1\">Loading Data and Model</a>\n",
    "2. <a href=\"#2\">Explanations</a> <br>\n",
    "    2.1. <a href=\"#21\">Saliency Maps</a> <br>\n",
    "    2.2. <a href=\"#22\">Integrated Gradients</a> <br>\n",
    "    2.3. <a href=\"#23\">SHAP Values</a> <br>\n",
    "    2.4. <a href=\"#24\">Counterfactual Explanations</a> <br>\n",
    "3. <a href=\"#3\">Summary</a> <br>\n",
    "\n",
    "---\n",
    "\n",
    "This notebook uses modified code snippets from [Captum](https://captum.ai/tutorials/Titanic_Basic_Interpret) and [PyTorch](https://pytorch.org/vision/stable/_modules/torchvision/models/resnet.html#resnet50.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operational libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "# Jupyter(lab) libraries\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reshaping/basic libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Explainability libraries\n",
    "from captum.robust import FGSM\n",
    "from captum.attr import IntegratedGradients, DeepLiftShap, GradientShap, Saliency\n",
    "from omnixai.data.image import Image\n",
    "from omnixai.explainers.vision import CounterfactualExplainer\n",
    "\n",
    "# Visualization libraries\n",
    "from utils.cv_utils import ImageNetCase\n",
    "from utils.viz_utils import visualize_image_attr_multiple\n",
    "\n",
    "# Store pretrained models and datasets\n",
    "cache_dir = Path(\".cache\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Neural Net libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow as tf\n",
    "\n",
    "# Globals\n",
    "import logging\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Loading Data and Model</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first step, let's load the `ImageNetCase` class which contains helper functions and methods that can be used to quickly load sample images, perform transformations and create predictions using a pre-trained version of ResNet that was customized to contain unique model names (which is a requirement for certain classes in Captum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set index for image to retrieve\n",
    "img_index = 1\n",
    "\n",
    "# instantiate image case\n",
    "ic = ImageNetCase()\n",
    "\n",
    "# load image based on selected index\n",
    "ic.load_image(img_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will present several different explanation methods. zto quickly extract the attribution/importance scores let's define a generic function, `attribute_scores_img`, that we can reuse. The attribution score of a feature indicates how much that feature contributed to the model's prediction. A positive score means that the feature contributed positively, while a negative score means that the feature contributed negatively. The magnitude of the score indicates the strength of the contribution. A zero score means that the feature had no contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_scores_img(algorithm, input_instance, **kwargs):\n",
    "    ic._model.zero_grad()\n",
    "    tensor_attributions = algorithm.attribute(\n",
    "        input_instance, target=img_index, **kwargs\n",
    "    )\n",
    "    return tensor_attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before investigating any of the explanations, let's have a look at the prediction that ResNet creates for this particular image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logits for prediction - to return probabilities, use return_probabilities=True\n",
    "logits = ic.predict()\n",
    "\n",
    "print(\n",
    "    \"Predicted\",\n",
    "    # lookup for class name\n",
    "    ic.class_names[np.argmax(logits).item()],\n",
    "    \"with logit:\",\n",
    "    # extract max logit value\n",
    "    round(logits.squeeze()[np.argmax(logits).item()].item(), 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our current input image and apply the transformation that is required to prepare the image (e.g., resizing, converting to a tensor, ...) for ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image to tensor\n",
    "input = ic._transform(ic._current_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the transformation, the image was also cropped and normalized. Eventually we will want to plot the model explanation next to the original image. However, the original image will be of different size. Therefore, to be able to plot a cropped version of the image that is true to the color of the original picture, we want to perform an inverse transformation and transpose the color channels and dimensions of the image to obtain the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert transformed image back to original (but maintain crop/size)\n",
    "original_image = np.transpose(\n",
    "    ic._inverse_transform(input).cpu().detach().numpy().squeeze(), (1, 2, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Explanations</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. <a name=\"21\">Saliency Maps</a>\n",
    "(<a href=\"#2\">Go to Explanations</a>)\n",
    "\n",
    "Saliency maps are visualizations of images in which the most important (salient) pixels are highlighted. Saliency maps (as introduced by Simonyan et al. [1]) highlight the areas of a given image, discriminative with respect to certain class. Because the expression \"saliency maps\" is often used to refer to the collection of approaches that create explanations for image problems, the method developed by Simonyan et al. is also called \"Vanilla Gradient\".\n",
    "\n",
    "To obtain the Vanilla Gradient attribution scores, a back-propagation with respect to the input image is performed (computing the gradient of the output with respect to the input image). The gradient quantifies the amount by which the classification score will change if the pixel changes by a small amount. \n",
    "\n",
    "It is also possible to calculate the saliency map of an image to a class other than its label; in that case the saliency map highlights which pixels detract or add to the classification.\n",
    "\n",
    "---\n",
    "[1] Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. \"Deep inside convolutional networks: Visualising image classification models and saliency maps.\" (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach gradients\n",
    "input.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Saliency method\n",
    "sal = Saliency(ic._model)\n",
    "\n",
    "# calculate pixel attributions\n",
    "attr_sal = attribute_scores_img(\n",
    "    sal,\n",
    "    input,\n",
    ")\n",
    "\n",
    "# reshape result for plotting\n",
    "attr_sal = np.transpose(attr_sal.squeeze().cpu().detach().numpy(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attributions\n",
    "_ = visualize_image_attr_multiple(\n",
    "    attr_sal,\n",
    "    original_image,\n",
    "    [\"original_image\", \"heat_map\"],\n",
    "    [\"all\", \"positive\"],\n",
    "    show_colorbar=True,\n",
    "    titles=[\"Original (cropped) image\", \"Saliency\"],\n",
    "    cmap=plt.cm.hot,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">\n",
    "<i class=\"fa fa-exclamation-circle\" style=\"color:red\"></i> Vanilla Gradient saliency maps can be problematic when certain activation functions are used, such as ReLU. For activation values below zero, ReLU applies a zero-cap. This is the so-called saturation problem; once the activation is saturated, the gradient will be zero and the pixels will be considered not important. Integrated gradients avoids this problem by integrating over a path, which means that it takes into account the entire activation function, not just the final output.\n",
    "\n",
    "<i class=\"fa fa-exclamation-circle\" style=\"color:red\"></i> Ghorbani et al. [2] showed that introducing small (adversarial) perturbations to an image, which still lead to the same prediction, can lead to very different pixels being highlighted as explanations. To illustrate this, review the example below. The example illustrates that pixel attribution methods can be very fragile. The issue of fragility also applies to other widely-used interpretation methods such as relevance propagation, and DeepLIFT. \n",
    "\n",
    "<i class=\"fa fa-exclamation-circle\" style=\"color:red\"></i> Kindermans et al. [3] showed that gradient-based methods can be unreliable by adding a constant shift to the input data. They compared two networks, the original network and a shifted network that had its bias adjusted to compensate for the constant shift. Both networks produced the same predictions, and their gradients were the same. However, the explanations generated by the two networks were different. This shows that gradient-based methods can be sensitive to changes in the input data, which can make them unreliable.\n",
    "\n",
    "<i class=\"fa fa-exclamation-circle\" style=\"color:red\"></i> The assumption is that pixels highlighted in saliency maps are the evidence for why a certain prediction was made. If that is indeed the case, when the prediction changes, the explanation should change.\n",
    "Therefore, if the prediction is random, the explanation should really change. It is possible to test this behavior as outlined in [Sanity Check for Saliency Maps](https://arxiv.org/abs/1810.03292) [4]. A simple test would be to randomize the weights of a model starting from the top layer, successively, all the way to the bottom layer. This will destroys the learned weights from the top layers to the bottom ones.\n",
    "\n",
    "---\n",
    "[2] Ghorbani, Amirata, Abubakar Abid, and James Zou. “Interpretation of neural networks is fragile.” Proceedings of the AAAI Conference on Artificial Intelligence. (2019). <br>\n",
    "[3] Kindermans, Pieter-Jan, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. “The (un) reliability of saliency methods.” (2019). <br>\n",
    "[4] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity checks for saliency maps. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS'18). Curran Associates Inc., Red Hook, NY, USA, 9525–9536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create version of current image with added random noise\n",
    "input_noise = ic._transform_noise(ic._current_image).unsqueeze(0)\n",
    "\n",
    "# convert back to original colors and clip RGB range\n",
    "original_input_noise = np.transpose(\n",
    "    torch.clip(ic._inverse_transform(input_noise[0]), min=0, max=1).detach().numpy(),\n",
    "    (1, 2, 0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pixel attributions\n",
    "attr_sal_noise = attribute_scores_img(\n",
    "    sal,\n",
    "    input_noise,\n",
    ")\n",
    "\n",
    "# reshape result for plotting\n",
    "attr_sal_noise = np.transpose(\n",
    "    attr_sal_noise.squeeze().cpu().detach().numpy(), (1, 2, 0)\n",
    ")\n",
    "\n",
    "# visualize attributions\n",
    "_ = visualize_image_attr_multiple(\n",
    "    attr_sal_noise,\n",
    "    original_input_noise,\n",
    "    [\"original_image\", \"heat_map\"],\n",
    "    [\"all\", \"positive\"],\n",
    "    show_colorbar=True,\n",
    "    titles=[\"Original (cropped) image\", \"Saliency map (pertubation)\"],\n",
    "    cmap=plt.cm.hot,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, adding noise can create a more smooth-looking saliency map (especially when using a random noise baseline). This happens because in images, there is a rich correlation structure between neighboring pixels. Once the model has learned the value of a pixel, it will not use nearby pixels as those will have similar intensities.\n",
    "\n",
    "However, by introducing random noise from an independent gaussian distribution this correlation structure will be broken up. This means that the importance of each pixel will be considered independently of the other pixel values and the resulting saliency map can look less noisy. \n",
    "\n",
    "Let's also have a look at whether the explanation is sensitive to the model itself by performing a mode randomization test. For this test we randomize the weights of a model starting from the top layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_randomization = copy.deepcopy(ic.model)\n",
    "\n",
    "# randomize model weights for top layer\n",
    "with torch.no_grad():\n",
    "    model_randomization.fc.weight = torch.nn.Parameter(\n",
    "        torch.randn(model_randomization.fc.weight.size()) * 0.02\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Saliency method\n",
    "sal_rand = Saliency(model_randomization)\n",
    "\n",
    "# calculate pixel attributions\n",
    "attr_sal_model_rand = attribute_scores_img(\n",
    "    sal_rand,\n",
    "    input,\n",
    ")\n",
    "\n",
    "# reshape result for plotting\n",
    "attr_sal_model_rand = np.transpose(\n",
    "    attr_sal_model_rand.squeeze().cpu().detach().numpy(), (1, 2, 0)\n",
    ")\n",
    "\n",
    "# visualize attributions\n",
    "_ = visualize_image_attr_multiple(\n",
    "    attr_sal_model_rand,\n",
    "    attr_sal,\n",
    "    [\"heat_map\", \"heat_map\"],\n",
    "    [\"positive\", \"positive\"],\n",
    "    show_colorbar=True,\n",
    "    titles=[\"Saliency map\", \"Saliency map (random predictions)\"],\n",
    "    cmap=plt.cm.hot,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, input):\n",
    "    # forward pass to calculate predictions\n",
    "    preds = model(input)\n",
    "\n",
    "    # get max logit\n",
    "    proba, indx = torch.max(preds, 1)\n",
    "\n",
    "    return ic.class_names[indx], proba\n",
    "\n",
    "\n",
    "get_prediction(model_randomization, input)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the model is now making random predictions; yet the saliency map remained mostly unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. <a name=\"22\">Integrated Gradients</a>\n",
    "(<a href=\"#2\">Go to Explanations</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Gradients are a local attribution method that computes the integral of the gradients of the output of the model for the predicted class with respect to the input image pixels along the path from a baseline image to the original input image. As the model gets more information, the prediction score changes in a meaningful way. By accumulating gradients along the path, the model gradient can be used to determine which input features contribute most to the model prediction.\n",
    "\n",
    "By integrating over a path, the Integrated Gradients method mitigates the saturation problem: The pixels' local gradients are accumulated when integrating along a straight line path from the baseline image to the input image.\n",
    "\n",
    "In practice, this integration is approximated with $k$ linearly-spaced points between 0 and 1 for some value of $k$. We can use this property to better understand the difference between the total approximated and true integrated gradients (approximation error). To estimate the error, we calculate integrated gradients for different values of $k$ points, and measure the difference. If the difference is big (large approximation error), then a larger $k$ is required. In general, the lower the absolute value of the convergence delta the better is the approximation. \n",
    "\n",
    "The difference between total approximated and true integrated gradients is also known as 'convergence delta'. The convergence delta can be returned by passing `return_convergence_delta = True` to the method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate IntegratedGradients method\n",
    "ig = IntegratedGradients(ic._model)\n",
    "\n",
    "# calculate pixel attributions\n",
    "attr_ig, delta = attribute_scores_img(\n",
    "    ig, input, baselines=input * 0, return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# reshape result for plotting\n",
    "attr_ig = np.transpose(attr_ig.squeeze().cpu().detach().numpy(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attributions\n",
    "_ = visualize_image_attr_multiple(\n",
    "    attr_ig,\n",
    "    original_image,\n",
    "    [\"original_image\", \"heat_map\"],\n",
    "    [\"all\", \"all\"],\n",
    "    show_colorbar=True,\n",
    "    titles=[\"Original (cropped) image\", \"Integrated Gradients\"],\n",
    "    cmap=sns.color_palette(\"bwr\", as_cmap=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">\n",
    "<i class=\"fa fa-exclamation-circle\" style=\"color:red\"></i> Integrated Gradient may produce spurious or noisy pixel attributions that aren’t related to the model’s predicted class. This is partly due to the accumulation of noise from regions of correlated, high-magnitude gradients for irrelevant pixels that occur along the straight line path that is used when computing Integrated Gradients.\n",
    "\n",
    "<i class=\"fa fa-exclamation-circle\" style=\"color:red\"></i> It is important to choose a good baseline for Integrated Gradient to make sensible feature attributions. For example, if a black image is chosen as baseline, Integrated Gradient won’t attribute importance to a completely black pixel in an actual image. The baseline value should both have a near-zero prediction, and also faithfully represent a complete absence of signal. Try a different baseline below to check how the attribution scores change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. <a name=\"23\">SHAP</a>\n",
    "(<a href=\"#2\">Go to Explanations</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientShap\n",
    "GradientShap is a gradient method to compute SHAP values. GradientShap combines ideas from Integrated Gradients, SHAP, and SmoothGrad - it can be viewed as an approximation of integrated gradients by computing the expectations of gradients for different baselines. Gaussian noise is added to each input sample multiple times, then a random point on the path between the baseline and the input is picked to determine the gradient of the outputs.\n",
    " \n",
    "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">\n",
    "<i class=\"fa fa-exclamation-circle\" style=\"color:red\"></i> GradientShap makes an assumption that the input features are independent and that the explanation model is linear, meaning that the explanations are modeled through the additive composition of feature effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GradientShap method\n",
    "gs = GradientShap(ic._model)\n",
    "\n",
    "# calculate pixel attributions\n",
    "attr_gs, delta = attribute_scores_img(\n",
    "    gs, input, baselines=input * 0, return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# reshape result for plotting\n",
    "attr_gs = np.transpose(attr_gs.squeeze().cpu().detach().numpy(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attributions\n",
    "_ = visualize_image_attr_multiple(\n",
    "    attr_gs,\n",
    "    original_image,\n",
    "    [\"original_image\", \"heat_map\"],\n",
    "    [\"all\", \"all\"],\n",
    "    show_colorbar=True,\n",
    "    titles=[\"Original (cropped) image\", \"GradientShap\"],\n",
    "    cmap=sns.color_palette(\"bwr\", as_cmap=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepLiftShap\n",
    "DeepLiftShap is a method extending DeepLIFT to approximate SHAP values, which are based on Shapley values proposed in cooperative game theory. DeepLIFT SHAP takes a distribution of baselines and computes the DeepLIFT attribution for each input-baseline pair and averages the resulting attributions per input example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate method\n",
    "dls = DeepLiftShap(ic._model)\n",
    "\n",
    "# calculate pixel attributions\n",
    "attr_dls, delta = attribute_scores_img(\n",
    "    dls,\n",
    "    input,\n",
    "    baselines=torch.cat([input * 0, input * 1]),\n",
    "    return_convergence_delta=True,\n",
    ")\n",
    "\n",
    "# reshape result for plotting\n",
    "attr_dls = np.transpose(attr_dls.squeeze().cpu().detach().numpy(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attributions\n",
    "_ = visualize_image_attr_multiple(\n",
    "    attr_dls,\n",
    "    original_image,\n",
    "    [\"original_image\", \"heat_map\"],\n",
    "    [\"all\", \"all\"],\n",
    "    show_colorbar=True,\n",
    "    titles=[\"Original (cropped) image\", \"DeepLiftShap\"],\n",
    "    cmap=sns.color_palette(\"bwr\", as_cmap=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. <a name=\"24\">Counterfactual (and adverserial) examples</a>\n",
    "(<a href=\"#2\">Go to Explanations</a>)\n",
    "Just like with tabular data, it is possible to create counterfactuals for images. Given a sample image, $x$, for which a model predicts predicts a certain class $c$, a counterfactual visual explanation identifies how $x$ could change such that the model would output a different specified class $c*$.\n",
    "\n",
    "To find the counterfactual, we need to find a distractor image, $x*$, for which the model predicts $c*$ and identify regions in the distractor image such that replacing said region in the original image, $x$, flips the prediction. Replacing regions can be achieved with a permutation matrix or by obfuscating certain parts of the image.\n",
    "\n",
    "Simplified we are trying to answer which parts of the image, if they are exchanged/not seen by the classifier, would most change the classifiers' decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the counterfactuals using the array of images provided above; we will create a counterfactual for the image at index zero, which is corresponds to the class `goldfish`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stack images in array, we want to resize them first to avoid length mismatch\n",
    "transform_stack = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1 / 229, 1 / 224, 1 / 225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# use current image as placeholder first image\n",
    "img_array = transform_stack(ic._current_image).unsqueeze(0).detach().numpy().astype(int)\n",
    "\n",
    "# loop through additional images\n",
    "for i in range(5):\n",
    "    i = transform_stack(ic.load_image(i)).unsqueeze(0).detach().numpy().astype(int)\n",
    "    img_array = np.concatenate((img_array, i), 0)\n",
    "\n",
    "# store data in PIL array\n",
    "pil_array = Image(data=img_array, batched=True, channel_last=False)\n",
    "\n",
    "# look at a cropped example\n",
    "pil_array[0].to_pil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell can take about 15 minutes to run; to reduce the wait time, specify one index, `idx`, to explain in `explainer.explain(pil_array[idx])` rather than getting counterfactuals for all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_process = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "pre = lambda ims: torch.stack([transform_process(im.to_pil()) for im in ims])\n",
    "\n",
    "\n",
    "# initialize explainer\n",
    "explainer = CounterfactualExplainer(\n",
    "    model=ic.model, preprocess_function=pre, num_iterations=50\n",
    ")\n",
    "\n",
    "# explain the test image\n",
    "explanations = explainer.explain(pil_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index for image to explain\n",
    "expl_idx = 5\n",
    "\n",
    "# show explanation visual for first image in sample array\n",
    "explanations.ipython_plot(index=int(expl_idx), class_names=ic._class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show cropped counterfactual for comparison\n",
    "Image(\n",
    "    data=transform_stack(\n",
    "        ic.load_image(int(explanations.explanations[expl_idx][\"cf_label\"]))\n",
    "    )\n",
    "    .detach()\n",
    "    .numpy(),\n",
    "    channel_last=False,\n",
    ").to_pil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method that can be used to create counterfactuals for image data is connected to the concept of adverserials: The same method that creates adversarial examples (AEs) to fool image-classifiers can be used to generate counterfactual explanations (CEs) that explain algorithmic decisions.\n",
    "\n",
    "We first utilize the Fast Gradient Sign Method (FGSM) to construct an adversarial example. FGSM utilizes the sign of the gradient to perturb the input: For a given input image, FGSM uses the gradients of the loss with respect to the input image to create a new image that maximizes the loss. This can be used to find counterfactuals. Because the gradients are used, this method is computationally very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct FGSM attacker\n",
    "fgsm = FGSM(ic.model, lower_bound=-1, upper_bound=1)\n",
    "\n",
    "perturbed_image_fgsm = fgsm.perturb(input, epsilon=0.2, target=img_index)\n",
    "\n",
    "new_pred_fgsm, score_fgsm = get_prediction(ic.model, perturbed_image_fgsm)\n",
    "\n",
    "print(new_pred_fgsm + \" \" + str(score_fgsm.item()))\n",
    "\n",
    "plt.imshow(\n",
    "    ic._inverse_transform(perturbed_image_fgsm)\n",
    "    .squeeze()\n",
    "    .permute(1, 2, 0)\n",
    "    .detach()\n",
    "    .numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. <a name=\"3\">Summary</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we looked at several different methods to create variations of pixel attribution maps. We found that pixel attribution maps are useful because they create explanations that are visual which makes it easy to immediately recognize the important regions of an image. In particular, we reviewed examples of gradient based methods (Vanilla Gradient, Integrated Gradients) and perturbation methods (GradientShap, DeepLiftShap).\n",
    "\n",
    "We found that various explanation methods also face certain challenges:\n",
    "- Pixel attribution methods can be very fragile by adding random noise to an image which did not impact the model prediction but changed the highlighted areas.\n",
    "- Saliency methods can be insensitive to model and data; by changing weights in the layers of our model we found that the prediction changed, yet the explanation did not.\n",
    "- Baselines need to be considered carefully as different choices for baselines in gradient based methods, will yield different results.\n",
    "\n",
    "When choosing a method to use, first and foremost it will be important to understand whether not you have access to the model itself. The methods presented in this notebook required direct access to the model. In addition, you should evaluate whether the method you want to use is sensitive to (various kinds of) perturbations, and whether or not changing the most salient pixels (insertion/deletion game) has an impact. For more details about metrics you can have a look at \"[Explaining Classifiers using Adversarial Perturbations on the Perceptual Ball](https://arxiv.org/pdf/1912.09405.pdf)\", and \"[Understanding Deep Networks via Extremal Perturbations and Smooth Masks](https://arxiv.org/pdf/1910.08485.pdf)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you for participating!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdd",
   "language": "python",
   "name": "kdd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
